<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no, maximum-scale=1.0">
    <title>Real-Time AI Tennis Coach</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
        }
        #video-container {
            position: relative;
            width: 100%;
            /* Set a large, fixed viewport height. This forces the container to be tall. */
            height: 75vh; 
            border-radius: 0.75rem;
            overflow: hidden;
            box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05);
            background-color: #000;
        }
        #video {
            display: block;
            width: 100%;
            height: 100%;
            object-fit: cover; /* Fill the container without stretching */
        }
        #status-overlay {
            position: absolute;
            bottom: 0;
            left: 0;
            right: 0;
            background: rgba(0, 0, 0, 0.6);
            color: white;
            padding: 0.75rem 1rem;
            text-align: center;
            font-weight: 500;
            transition: opacity 0.3s ease-in-out;
        }
        .pulsing {
            animation: pulse 2s cubic-bezier(0.4, 0, 0.6, 1) infinite;
        }
        @keyframes pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.7; }
        }
        #modal-backdrop { transition: opacity 0.3s ease-in-out; }
        #modal-content { transition: transform 0.3s ease-in-out; }
    </style>
</head>
<body class="bg-gray-100 text-gray-800">

    <div class="w-full max-w-4xl mx-auto p-4">
        <div class="text-center mb-4">
            <h1 class="text-2xl sm:text-4xl font-bold text-gray-900">AI Tennis Forehand Coach</h1>
            <p class="text-md sm:text-lg text-gray-600 mt-1">Real-time feedback powered by Gemini.</p>
        </div>

        <div id="video-container">
            <video id="video" playsinline autoplay></video>
            <canvas id="canvas" class="hidden"></canvas>
            <div id="status-overlay" class="opacity-0">
                <p id="status-text">Initializing...</p>
            </div>
        </div>

        <div class="flex flex-col items-center space-y-4 w-full mt-4">
            <div class="w-full max-w-md bg-white p-4 rounded-lg shadow-md">
                <label for="api-key-input" class="block text-sm font-medium text-gray-700 mb-1">Enter Your Gemini API Key:</label>
                <div class="flex space-x-2">
                    <input type="password" id="api-key-input" class="flex-grow p-2 border border-gray-300 rounded-lg focus:ring-blue-500 focus:border-blue-500" placeholder="Paste your API key here">
                    <button id="save-key-button" class="px-4 py-2 bg-blue-600 text-white font-semibold rounded-lg hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-blue-500">Save Key</button>
                </div>
                <p id="api-key-status" class="text-xs text-gray-500 mt-2">You need a free Gemini API key from <a href="https://aistudio.google.com/apikey" target="_blank" class="text-blue-600 hover:underline">Google AI Studio</a> to use this app.</p>
            </div>
            
            <div class="flex items-center space-x-4">
                 <button id="toggle-button" class="px-8 py-4 bg-blue-600 text-white font-bold rounded-lg shadow-lg hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-blue-500 focus:ring-opacity-50 transition-transform transform hover:scale-105 disabled:bg-gray-400 disabled:cursor-not-allowed disabled:scale-100">
                    Start Coaching
                </button>
                <button id="flip-camera-button" class="hidden p-4 bg-gray-200 text-gray-700 rounded-full shadow-lg hover:bg-gray-300 focus:outline-none focus:ring-2 focus:ring-gray-400">
                    <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2">
                        <path stroke-linecap="round" stroke-linejoin="round" d="M4 4v5h5m7 10v-5h-5M6.343 17.657A8 8 0 1117.657 6.343 8 8 0 016.343 17.657z" />
                    </svg>
                </button>
            </div>

             <button id="summary-button" class="hidden px-8 py-4 bg-purple-500 text-white font-bold rounded-lg shadow-lg hover:bg-purple-600 focus:outline-none focus:ring-2 focus:ring-purple-500 focus:ring-opacity-50 transition-transform transform hover:scale-105">
                ‚ú® Summarize My Session
            </button>
            <div id="feedback-container" class="w-full bg-white p-4 rounded-lg shadow-md min-h-[80px] flex items-center justify-center">
                <p id="feedback-text" class="text-gray-500 text-center">Your coaching advice will appear here.</p>
            </div>
        </div>
    </div>

    <!-- Modal Structure -->
    <div id="modal" class="hidden fixed inset-0 z-50 flex items-center justify-center p-4">
        <div id="modal-backdrop" class="fixed inset-0 bg-black bg-opacity-50"></div>
        <div id="modal-content" class="bg-white rounded-lg shadow-2xl w-full max-w-2xl max-h-[80vh] overflow-y-auto p-6 z-10 transform scale-95">
            <div class="flex justify-between items-center border-b pb-3 mb-4">
                <h2 id="modal-title" class="text-2xl font-bold"></h2>
                <button id="modal-close" class="text-gray-500 hover:text-gray-800 text-3xl">&times;</button>
            </div>
            <div id="modal-body" class="prose max-w-none text-gray-700 whitespace-pre-wrap"></div>
        </div>
    </div>

    <script>
        // --- DOM Elements ---
        const video = document.getElementById('video');
        const canvas = document.getElementById('canvas');
        const toggleButton = document.getElementById('toggle-button');
        const summaryButton = document.getElementById('summary-button');
        const flipCameraButton = document.getElementById('flip-camera-button');
        const statusOverlay = document.getElementById('status-overlay');
        const statusText = document.getElementById('status-text');
        const feedbackText = document.getElementById('feedback-text');
        const modal = document.getElementById('modal');
        const apiKeyInput = document.getElementById('api-key-input');
        const saveKeyButton = document.getElementById('save-key-button');
        const apiKeyStatus = document.getElementById('api-key-status');

        // --- State and Configuration ---
        let stream;
        let coachingTimeout = null;
        let isCoaching = false;
        let sessionFeedbackLog = [];
        let userApiKey = '';
        let hasMultipleCameras = false;
        let currentFacingMode = 'environment';
        let audioContext;
        let gainNode;
        const CAPTURE_INTERVAL = 10000;

        // --- Gemini API Configuration ---
        const TEXT_API_URL = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-05-20:generateContent`;
        const TTS_API_URL = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-tts:generateContent`;
        const VISION_PROMPT = `You are an expert tennis coach analyzing a tennis player's form from a single image. Your analysis should be based on the 8 phases of a world-class stroke: Split Step, Positioning, Unit Turn, Back Swing, Acceleration, Contact, Follow Through, and Recovery. After analyzing all 8 phases, your task is to identify the single most critical area for improvement. Your entire output MUST be a single, concise sentence that summarizes this one key piece of advice. This advice must be a maximum of 2 sentences and under 30 words. For example: 'Focus on extending your wrist more during the backswing for better racket lag.' or 'Your unit turn looks great, but try to get your contact point further in front.' If you cannot identify any clear flaw or if the player's form looks good, do not return any text. Do not provide the full 8-point breakdown in your response.`;

        // --- Event Listeners ---
        toggleButton.addEventListener('click', toggleCoaching);
        flipCameraButton.addEventListener('click', flipCamera);
        summaryButton.addEventListener('click', () => openModal(summarizeSession(), "üìù Session Summary"));
        document.getElementById('modal-close').addEventListener('click', closeModal);
        document.getElementById('modal-backdrop').addEventListener('click', closeModal);
        saveKeyButton.addEventListener('click', saveApiKey);
        document.addEventListener('DOMContentLoaded', initializeApp);

        // --- Core Functions ---
        function initializeApp() {
            const savedKey = localStorage.getItem('gemini-api-key');
            if (savedKey) {
                userApiKey = savedKey;
                apiKeyInput.value = savedKey;
                apiKeyStatus.textContent = 'API Key loaded from your browser.';
                apiKeyStatus.classList.add('text-green-600');
            }
            checkApiKeyState();
        }

        function saveApiKey() {
            const key = apiKeyInput.value.trim();
            if (key) {
                localStorage.setItem('gemini-api-key', key);
                userApiKey = key;
                apiKeyStatus.textContent = 'API Key saved successfully!';
                apiKeyStatus.classList.add('text-green-600');
                apiKeyStatus.classList.remove('text-red-600');
            } else {
                apiKeyStatus.textContent = 'Please enter a valid API key.';
                apiKeyStatus.classList.add('text-red-600');
                apiKeyStatus.classList.remove('text-green-600');
            }
            checkApiKeyState();
        }
        
        function checkApiKeyState() {
            toggleButton.disabled = !userApiKey.trim();
            if (!userApiKey.trim()) {
                feedbackText.textContent = 'Please enter and save your Gemini API key to begin.';
            } else if (!isCoaching) {
                feedbackText.textContent = 'Your coaching advice will appear here.';
            }
        }
        
        async function toggleCoaching() {
            if (isCoaching) stopCoaching();
            else await startCoaching();
        }

        async function startCamera() {
            if (stream) {
                stream.getTracks().forEach(track => track.stop());
            }
            const constraints = { video: { facingMode: currentFacingMode }, audio: true };
            try {
                updateStatus('Requesting camera & mic access...', true);
                stream = await navigator.mediaDevices.getUserMedia(constraints);
                video.srcObject = stream;
                video.muted = true; 
                await video.play();

                if (!audioContext || audioContext.state === 'closed') {
                    audioContext = new (window.AudioContext || window.webkitAudioContext)();
                    const microphoneSource = audioContext.createMediaStreamSource(stream);
                    gainNode = audioContext.createGain();
                    gainNode.gain.setValueAtTime(0, audioContext.currentTime);
                    microphoneSource.connect(gainNode);
                    gainNode.connect(audioContext.destination);
                }

                video.style.transform = currentFacingMode === 'user' ? 'scaleX(-1)' : 'scaleX(1)';
                updateStatus('Camera started.', true);
                if (!hasMultipleCameras) {
                    const devices = await navigator.mediaDevices.enumerateDevices();
                    hasMultipleCameras = devices.filter(d => d.kind === 'videoinput').length > 1;
                }
                if (hasMultipleCameras) flipCameraButton.classList.remove('hidden');
            } catch (error) {
                console.error('Error accessing camera/mic:', error);
                updateStatus('Camera/mic access denied. Please enable permissions.', false, true);
                throw error;
            }
        }

        async function flipCamera() {
            currentFacingMode = currentFacingMode === 'environment' ? 'user' : 'environment';
            await startCamera();
        }
        
        async function startCoaching() {
            isCoaching = true;
            sessionFeedbackLog = [];
            summaryButton.classList.add('hidden');
            toggleButton.textContent = 'Stop Coaching';
            toggleButton.classList.replace('bg-blue-600', 'bg-red-600');
            toggleButton.classList.replace('hover:bg-blue-700', 'hover:bg-red-700');
            feedbackText.textContent = "Let's begin! I'll provide feedback shortly.";
            try {
                await startCamera();
                updateStatus('Coaching started. Capturing first clip...', true, false, true);
                coachingLoop(); 
            } catch (error) {
                stopCoaching();
            }
        }

        function stopCoaching() {
            isCoaching = false;
            if (coachingTimeout) clearTimeout(coachingTimeout);
            if (stream) {
                stream.getTracks().forEach(track => track.stop());
                stream = null;
                video.srcObject = null;
            }
            if (audioContext && audioContext.state !== 'closed') {
                audioContext.close().then(() => audioContext = null);
            }
            if(sessionFeedbackLog.length > 0) summaryButton.classList.remove('hidden');
            toggleButton.textContent = 'Start Coaching';
            toggleButton.classList.replace('bg-red-600', 'bg-blue-600');
            toggleButton.classList.replace('hover:bg-red-700', 'hover:bg-blue-700');
            flipCameraButton.classList.add('hidden');
            updateStatus('Coaching stopped.', false);
            checkApiKeyState();
        }

        async function coachingLoop() {
            if (!isCoaching) return;
            try {
                updateStatus('Capturing your form...', true, false, true);
                const base64Image = captureFrame();
                if (base64Image) {
                    updateStatus('Analyzing your form with Gemini...', true, false, true);
                    const analysisText = await analyzeFrame(base64Image);
                    if (analysisText) {
                        feedbackText.textContent = analysisText;
                        sessionFeedbackLog.push(analysisText);
                        await speakFeedback(analysisText);
                    } else {
                         updateStatus(`Looking good! Next analysis in ${CAPTURE_INTERVAL / 1000}s.`, true);
                    }
                } else {
                    console.warn("Failed to capture frame, will retry.");
                    updateStatus('Waiting for camera...', true, false, true);
                }
            } catch (error) {
                console.error("Error in coaching loop:", error);
                updateStatus('An error occurred. Retrying soon.', true, true);
                feedbackText.textContent = "Sorry, I couldn't analyze that shot.";
            } finally {
                if (isCoaching) coachingTimeout = setTimeout(coachingLoop, CAPTURE_INTERVAL);
            }
        }
        
        function summarizeSession() {
            if (sessionFeedbackLog.length === 0) return "No feedback was recorded in this session.";
            
            const feedbackString = sessionFeedbackLog.join('. ');
            const prompt = `You are an encouraging tennis coach. Based on the following list of feedback given to a player during a practice session, write a brief summary. First, identify the most common area for improvement. Then, provide a short, positive message and one concrete tip for their next session. Use markdown for structure (e.g., ### Key Focus Area, ### Coach's Note). Feedback list: "${feedbackString}"`;
            
            openModal("Analyzing your session...", "üìù Session Summary");
            generateText(prompt)
                .then(summaryText => document.getElementById('modal-body').textContent = summaryText)
                .catch(error => {
                    console.error("Error summarizing session:", error);
                    document.getElementById('modal-body').textContent = "Sorry, I couldn't generate a summary right now.";
                });
        }
        
        async function generateText(prompt) {
            const payload = { contents: [{ parts: [{ text: prompt }] }] };
            const response = await fetch(`${TEXT_API_URL}?key=${userApiKey}`, { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify(payload) });
            if (!response.ok) throw new Error(`API Error: ${response.status} ${response.statusText}`);
            const result = await response.json();
            return result.candidates?.[0]?.content?.parts?.[0]?.text || "No response generated.";
        }

        function captureFrame() {
            if (!stream || !video.srcObject || video.readyState < 2) return null;
            const settings = stream.getVideoTracks()[0].getSettings();
            canvas.width = settings.width;
            canvas.height = settings.height;
            const context = canvas.getContext('2d');
            if (currentFacingMode === 'user') {
                context.scale(-1, 1);
                context.drawImage(video, -canvas.width, 0, canvas.width, canvas.height);
            } else {
                context.drawImage(video, 0, 0, canvas.width, canvas.height);
            }
            return canvas.toDataURL('image/jpeg', 0.8).split(',')[1];
        }

        async function analyzeFrame(base64ImageData) {
            const payload = { contents: [{ parts: [{ text: VISION_PROMPT }, { inlineData: { mimeType: "image/jpeg", data: base64ImageData } }] }] };
            const response = await fetch(`${TEXT_API_URL}?key=${userApiKey}`, { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify(payload) });
            if (!response.ok) throw new Error(`API Error: ${response.status} ${response.statusText}`);
            const result = await response.json();
            return result.candidates?.[0]?.content?.parts?.[0]?.text || null;
        }
        
        function speakFeedback(text) {
            if (!text || !audioContext) return Promise.resolve();
            updateStatus('Playing feedback...', true);
            return new Promise(async (resolve, reject) => {
                const payload = { contents: [{ parts: [{ text }] }], generationConfig: { responseModalities: ["AUDIO"], speechConfig: { voiceConfig: { prebuiltVoiceConfig: { voiceName: "Kore" } } } } };
                try {
                    const response = await fetch(`${TTS_API_URL}?key=${userApiKey}`, { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify(payload) });
                    if (!response.ok) throw new Error(`TTS API Error: ${response.status} ${response.statusText}`);
                    const result = await response.json();
                    const audioData = result.candidates?.[0]?.content?.parts?.[0]?.inlineData?.data;
                    const mimeType = result.candidates?.[0]?.content?.parts?.[0]?.inlineData?.mimeType;
                    if (!audioData) throw new Error("No audio data received");

                    const sampleRate = parseInt(mimeType.match(/rate=(\d+)/)[1], 10);
                    const pcmData = base64ToArrayBuffer(audioData);
                    const wavBlob = pcmToWav(new Int16Array(pcmData), 1, sampleRate);
                    const arrayBuffer = await wavBlob.arrayBuffer();

                    audioContext.decodeAudioData(arrayBuffer, (buffer) => {
                        const source = audioContext.createBufferSource();
                        source.buffer = buffer;
                        source.connect(audioContext.destination);
                        source.onended = () => {
                            if(isCoaching) updateStatus(`Next analysis in ${CAPTURE_INTERVAL / 1000}s.`, true);
                            resolve();
                        };
                        source.start(0);
                    }, (e) => reject(e));
                } catch (error) {
                    console.error("Failed to speak feedback:", error);
                    updateStatus('Error playing audio.', true, true);
                    reject(error);
                }
            });
        }
        
        function openModal(bodyText, titleText) {
            document.getElementById('modal-title').textContent = titleText;
            document.getElementById('modal-body').textContent = bodyText;
            modal.classList.remove('hidden');
            setTimeout(() => {
                document.getElementById('modal-backdrop').classList.remove('opacity-0');
                document.getElementById('modal-content').classList.remove('scale-95');
            }, 10);
        }

        function closeModal() {
            document.getElementById('modal-backdrop').classList.add('opacity-0');
            document.getElementById('modal-content').classList.add('scale-95');
            setTimeout(() => modal.classList.add('hidden'), 300);
        }

        function updateStatus(message, show, isError = false, isPulsing = false) {
            statusText.textContent = message;
            statusOverlay.classList.toggle('opacity-0', !show);
            statusOverlay.classList.toggle('bg-red-600', isError);
            statusOverlay.classList.toggle('bg-opacity-60', !isError);
            statusOverlay.classList.toggle('pulsing', isPulsing);
        }

        function base64ToArrayBuffer(base64) {
            const binaryString = atob(base64);
            const len = binaryString.length;
            const bytes = new Uint8Array(len);
            for (let i = 0; i < len; i++) {
                bytes[i] = binaryString.charCodeAt(i);
            }
            return bytes.buffer;
        }

        function pcmToWav(pcmData, numChannels, sampleRate) {
            const buffer = new ArrayBuffer(44 + pcmData.length * 2);
            const view = new DataView(buffer);
            const bitsPerSample = 16;
            const blockAlign = numChannels * bitsPerSample / 8;
            const byteRate = sampleRate * blockAlign;
            const writeString = (v, offset, s) => { for (let i = 0; i < s.length; i++) v.setUint8(offset + i, s.charCodeAt(i)); };
            writeString(view, 0, 'RIFF');
            view.setUint32(4, 36 + pcmData.length * 2, true);
            writeString(view, 8, 'WAVE');
            writeString(view, 12, 'fmt ');
            view.setUint32(16, 16, true);
            view.setUint16(20, 1, true);
            view.setUint16(22, numChannels, true);
            view.setUint32(24, sampleRate, true);
            view.setUint32(28, byteRate, true);
            view.setUint16(32, blockAlign, true);
            view.setUint16(34, bitsPerSample, true);
            writeString(view, 36, 'data');
            view.setUint32(40, pcmData.length * 2, true);
            for (let i = 0; i < pcmData.length; i++) {
                view.setInt16(44 + i * 2, pcmData[i], true);
            }
            return new Blob([view], { type: 'audio/wav' });
        }
    </script>
</body>
</html>

